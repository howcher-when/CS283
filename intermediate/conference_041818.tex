\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{siunitx}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
	
	\title{Proposal on Mapping Robot*\\
		{\footnotesize \textsuperscript{*}Note: This is a course project proposal for Robotics (CS283) in ShanghaiTech University in Fall 2019.}
	}
	
	\author{
		% \IEEEauthorblockN{Xiting Zhao}
		% \IEEEauthorblockA{\textit{School of Information Science and Technology} \\
		% \textit{ShanghaiTech University}\\
		% Shanghai, China \\
		% zhaoxt@shanghaitech.edu.cn}
		% \and
		% \IEEEauthorblockN{Zhijie Yang}
		% \IEEEauthorblockA{\textit{School of Information Science and Technology} \\
		% \textit{ShanghaiTech University}\\
		% Shanghai, China \\
		% yangzhj@shanghaitech.edu.cn}
		% \and
		% \IEEEauthorblockN{Haochuan Wan}
		% \IEEEauthorblockA{\textit{School of Information Science and Technology} \\
		% \textit{ShanghaiTech University}\\
		% Shanghai, China \\
		% wanhch@shanghaitech.edu.cn}
		Xiting Zhao\textsuperscript{1}, Zhijie Yang\textsuperscript{1} and Haochuan Wan\textsuperscript{1}
		\thanks{\textsuperscript{1}All authors are with the School of Information Science and Technology, 
			ShanghaiTech University, China.
			{\tt\small [zhaoxt, yangzhj, wanhch]@shanghaitech.edu.cn}}%
	}
	
	\maketitle
	
	\begin{abstract}
		This part of paper presents an approach to synchronizing different types of sensors which are installed on mapping robot.
	\end{abstract}
	
	\begin{IEEEkeywords}
		Sensor Synchronization
	\end{IEEEkeywords}
	
	\section{Introduction}
	For autonomous robots, the ability of getting from one place to another place is the most important capability. In order to realize this capability, Simultaneous Localization and Mapping (SLAM) system is often used and the result of SLAM is positively correlated with the quality of input data. To get better data, more sensors are used to collect information from environment. In our project, we installed an epic number of sensors, 10 RGB monocular cameras, 4 Lidars, 1 onmidirectional camera, 2 event cameras, 1 IMU and even IR cameras. However, as the number of sensors increases, the problem about synchronization between sensors is raised because synchronized system can lead sensors collect data at different time which results in errors in the generated map. In our project, we will migrate the existing synchronization on the existing mapping robot to the new one.
	
	\section{State of the Art}
	
	\subsection{Associated Datasets --- by Zhijie Yang}
	ICL-NUIM dataset \cite{ICL-NUIM} mainly focused on RGB-D camera with handheld perspective within simulated indoor environments. The ground truth of this dataset comes from the model of the room. 
	
	Robot@Home dataset \cite{Robot@Home} consists of the data stream comes from 4 RGB-D camera and 2 2D Lidar, but there is no pose ground truth. % But it is said to have pose ground truth via algorithm in the table?! 
	CoRBS dataset\cite{CoRBS} is the first to have ground truth of camera trajectory and 3D model of the scenes. The camera is also upgraded to Microsoft Kinect v2 compared to TUM-RGBD dataset launched in 2012. The RGB resolution is $1920\times 1080@$30Hz and the depth cloud also revolves at 30Hz and have $512\times 424$ pixels in each frame. The tracking system generates 3D pose ground truth at 120Hz with error of 0.39mm.
	
	Oxford RobotCar dataset\cite{RobotCar} is a dataset with four types of heterogeneous sensors mounted on a ground vehicle. The total mileage of this dataset covers more than 1000 kilometers. The scenes are within Oxford city, while most of them are repetition traverses along a route in Oxford. Its video stream consists of a stereo camera built up with three monocular RGB cameras, each equipped with a 2.67mm fisheye lens, obtaining \ang{180} field of view (FOV). The depth data are captured through two 2D Light Detection and Ranging sensors (Lidar) and a four-beam 3D Lidar. The two 2D Lidars are mounted to make the scans perpendicular to the ground, making it have a complete scan of the surroundings in a ``push-broom" manner. The 3D Lidar is mounted facing to the front, as a common configuration focusing the depth information in the front direction. The platform is also equipped with a 6 degree of freedom (DOF) IMU and GPS/GLONASS system reports its position at 50Hz. The three monocular cameras are synchronized at 11.1Hz frame rate, as the stereo camera produces 16Hz. The Lidar scans at 12.5Hz. All these sensors have their clock calibrated using TICSync. 
	
	\subsubsection{ROS Package: velodyne --- by Zhijie Yang}
	The ROS velodyne package is a set of packages consists of ``velodyne\_drive'', ``velodyne\_laserscan'', ``velodyne\_msgs'',  ``velodyne\_pointcloud'' and some utilities in directory ``velodyne''. The ``velodyne\_drive'' is the driver for the velodyne laser scanner. It is written in the ``node'' and ``nodelet'' manner. It deals with different models of velodyne Lidars and parse the TCP packets into scanning data. ``velodyne\_laserscan'' subscribes from topic ``velodyne\_points'' converts them into type of ``LaserScan'' and publishes them out. ``velodyne\_msgs'' defines the data structure of ``VelodynePacket'' and ``VelodyneScan'', where the former one has a time stamp and an 1206-element uint8 array as field for data, while the latter one has a ROS standard message header and an array of ``VelodynePacket'' with unpredefined length. The ``velodyne\_pointcloud'' packet is the one converts ``VelodynePackets'' into point clouds using pointcloud2 library. This combination of velodyne packages works as a whole --- the bottom layer driver acquires the TCP packets from via the ethernet, the laserscan package process the scans, the msgs package defines the messages types and the pointcloud package generats the ros-usable pointcloud2 point clouds. 
	
	This packages is also simple to use --- clone it from github (https://github.com/ros-drivers/velodyne) into the src directory of a catkin work space, run catkin\_make, source the setup.bash, and it is done. 
	
	\subsection{Synchronization --- by Wan Haochuan}
	In some previous works, most of approaches of synchronization just stayed on the level of software. Hang Hu \cite{1} mainly used ROS nodes to synchronize on software level. They realized the synchronization between cameras, LIDARs and GPS. However, the approach they provided focus on calibrating data rather synchronizing when sensors generating data.
	In \cite{Olson2010A}, a solution based on math calculation is introducted. It is a passive method to synchronize by eliminating transmission error between sensor data, which is a low-cost way to realize synchronzation.
	A system \cite{Amundson2006Time} is designed for heterogeneous sensor networks is used to synchronize time. In this work, authors designed both software and hardware for synchronization. Software guatantees sensors receive signal at the same time and hardware like phase-locked loop is to correct time when errors occur.
	\par
	\begin{figure}
		\centering
		\includegraphics[width=.4\textwidth]{kitti_car.png}
		\caption{KITTI car}
		\label{car}
	\end{figure}
	By Georgios Litos's work \cite{Litos2006Synchronous}, a software-based system is provided. This method concerns cameras which are installed on different computers, and these computers are connected with NTP to get correct time. In order to trigger cameras at the same time, an estimate $L$ of how much time is required for cameras data is broadcast to all computers. Though time error will occur when the message propagate, yet each computer can calculate the time to wait and trigger the camera at the right time. In this case, the system can be used in a long distance sensor synchronization with NTP. But disadvantages are also existed if this system is applyed on mapping robot directly. First, the distance between cameras on mapping robot is much closer so that to trigger different cameras with different copmuters is a waste. Second, this method can just synchronize trigger sensors, like camera and IMU. But there is another type of sensor by timestamp like pps and NMEA data which cannot be triggered, such as LIDRA and GPS. Third, the NTP can get a millisecond accuracy, which cannot meet the requirement of mapping robot in that some sensors like IMU should synchronized by a 200Hz signal. However, this work \cite{Litos2006Synchronous} provides a experiment which can evaluate the synchronization. They use a LED array controlled by a MircoChip. This array has 4 rows of 10 LEDs that can correspond to 1000ms, 100ms 10ms and 1ms granuilarities. Compared a 100Hz CRT screen which just provides 10ms accuracy, the LED array has an edge. By photographing the LED array, we can get the time of shot and judge the result of synchronization. This experiment method can evaluate some photosensitive sensor like cameras, but sensors such as IMU and LIDRA cannot be evaluated in this experiment.
	\par
	KITTI dataset \cite{Geiger2013IJRR} is one of the most famous opensource dataset regarding on autonomous driving. The main function of KITTI is collecting data from real world to build map for autonomous. Though the raw data is not in formal of ROS bag, yet we can convert it to rosbag easiliy by "kitti2bag" in python. And here we can use a ROS package which is "kitti\_to\_rosbag". By this ros package, KITTI dataset raw data can be converted to a ROS bag and allows a library for direct access to poses, velodyne scans and images. The car used by KITTI is equipped with a HDL-64E 3D Lidar, grayscale and RGB stereo cameras, which is show in Fig \ref{car}. All the Lidar and the cameras are synchronized and generates data at 10Hz. Global positioning system (GPS) and IMU data are also included in this dataset with 100Hz. The synchronization between GPS and IMU is not hardware-level synchronized with the camera or the Lidar. They use the time stamp to synchronize among them. There are six categories in KITTI and they are city, residential, road, campus, person and calibration.	KITTI dataset is also used to evaluate the performance of computer vision technology such as stereo, optical flow, visual odometry, 3D object detection and 3D tracking in the vehicle environment. KITTI contains real-world image data from scenes such as urban, rural, and highways, with up to 15 vehicles and 30 pedestrians per image, as well as varying degrees of occlusion and truncation. The entire data set consists of 389 pairs of stereo images and optical flow maps, 39.2km visual ranging sequences and images of 3D labeled objects, sampled and synchronized at 10Hz. For 3D object detection, the label is subdivided into car, van, truck, pedestrian, pedestrian (sitting), cyclist, tram and misc.
	\par
	In Xu Ding's work \cite{8074435}, they introduced a method based on LED arrays to measure errors between different cameras. They use a FPGA to control LED matrix to get a high precision method to detect the synchronization accuracy. The time of cameras capturing an image can be represented by the state of LED matrix. According to the capture time, the relative time interval between the cameras can be calculated. However, in their work, they only realized 8ms accuracy, which is not enough to measure our system where the highest frequence is up to 200Hz.
	\subsection{Software --- by Xiting Zhao}
	There are some previous dataset, for example.
	TUM-RGBD dataset \cite{TUM-RGBD}, as another indoor dataset, is captured by a synchronized Microsoft Kinect RGB-D camera both handheld and onboard of a ground robot. It has pose ground truth coming from a motion capture system at 100Hz. The images are with $640\times 480@30$Hz. 
	EuRoC dataset\cite{EuRoC} is from a micro aerial vehicle (MAV). It is equipped with synchronized stereo monochrome cameras at 20 fps, inertia measurement unit (IMU) and has pose ground truth from laser tracking systems.
	Zurich Urban MAV dataset \cite{Zurich_Urban_MAV} is also based on MAVs. Different from EuRoC, it is recorded in outdoor environments. The data are also time synchronized.
	
	Zurich Urban MAV dataset use the MAV capture system to record the data in urban streets of Zurich, Switzerland. They fly the MAV at 5-15 meters above the ground. Their capture system is on a Fotokite quadrotor with a lot of sensors including barometer, gyrometer, accelerometer and GPS receiver synchronized with autopilot board and the cameras. It record the image at 1920 x 1080 and 30 fps. It use the GoPro Hero 4 to record the image. To get the ground truth pose of MAV, they performed an accurate photogrammetric 3D reconstruction using the Pix4D software. The GPS location is used for initial position in its ground truth pose calculation algorithm. The accuracy of the GPS pose is not so accurate because of the shadow of the building in the urban area. After that, they use the bundle adjustment algorithm with huge amount of 2D and 3D camera point to optimize the pose result and get a more accurate pose as the ground truth. So its ground truth comes from algorithms instead of directly from the capture system. In their dataset, they also have the ground Google Street View images which may be useful for the matching between the MAV image and the ground image. They also provide the calibration of the sensors including the Camera, the IMU, the GPS and the body of MAV. Their dataset are recorded in the raw data, but they provide a parsing algorithm to replay the data to the ROS platform.
	
	We will use the ROS image\_transport package. The image\_transport package is used to subscribe to and publish images. When it subscribe or publish the image, it use the image\_transport\_plugin to compress and uncompress the image. In default, it provide three image\_transport\_plugin, which are the JPEG compression, the PNG compression and the Theora streaming video. The JPEG and PNG compression image\_transport\_plugin just use the OpenCV JPEG and PNG encoder and decoder to process the subscribed image. The OpenCV JPEG encoder and decoder use the libjpeg or the libjpeg-turbo library. The OpenCV PNG encoder and decoder use the libpng library. However all these image\_transport\_plugin are single thread and are very slow. According to the test, it can compress the 2448 x 2048 image at less than 50 fps at JPEG compression quality 90, when working on Intel i7 8700 at single thread. When it comes to the PNG image\_transport\_plugin, it can only compress the 2448 x 2048 image at less than 5 fps, when working on Intel i7 8700 at single thread. The most slow is the Theora image\_transport\_plugin, it can only compress the 2448 x 2048 image at less than 1 fps, when working on Intel i7 8700 at single thread. But for the date rate, the Theora as a video compressing format has the lowest data rate. The JPEG has a bit higher than that, as it is a compression with lost of the information. The PNG compression has the highest data rate.
	
	\begin{figure}
		\centering
		\includegraphics[width=.4\textwidth]{pcb.jpg}
		\caption{PCB for singal amplification}
		\label{pcb}
	\end{figure}
	
	\begin{figure}
		\centering
		\includegraphics[width=.4\textwidth]{wireless.jpg}
		\caption{Wireless singal transmitter for tracking system}
		\label{wireless}
	\end{figure}
	
	\begin{figure}
		\centering
		\includegraphics[width=.4\textwidth]{topology.png}
		\caption{Trigger \& Timestamp Spread Topology}
		\label{topology}
	\end{figure}
	
	\section{System Description}
	On our machine, there are two different classes deviced to be synchronized. One is triggered devices and GPS-stamp devices. For triggered devices (e.g. cameras and IMUs), we need to generate a square wave (its initial phase is aligned with the second boundary) and pass the wave to the device as a trigger signal. For another GPS timestamp device (e.g. Velodyne Lidars), we use a 1Hz square wave, whose duty cycle is adapted to the specified target device as PPS through GPIO (General Purpose Input Output), and the timestamp is encoded as GPRMC (NMEA 2.0) Sentence) format via UART (Universal Asynchronous Receiver / Transmitter). The main problem of synchronization is to generate two different types of signals and ensure the delay between each signals are acceptable.
	\par
	The core of the synchronization is TinkerBoard which broadcast triggering signals and NMEA message for two different kinds of sensors ------ Triggered devices and GPS devices. Triggered devices includes cameras at 10Hz, tracking system at 30Hz and IMU at 200Hz; GPS devices are LIDRAs, Velodyne HDL-32E, which is synchronized by a PPS and message of \$GPRMC every second. The clock of TinkerBorad is synchronized by NTP with the host. To control different cameras with same signal, a PCB in Fig \ref{pcb} is designed to broadcase signal from one signal and convert 3.3V to 5V. Considering synchronization with Optitrack tracking system, a wireless signal transmitter is designed based on Arduino in Fig \ref{wireless}. The whole system is a hardware-level synchronization and showed in Fig \ref{topology}.
	\paragraph{Details for synchronization}
	There are 2 programs in \textit{/src} of the synchronization ROS package, which are \textit{"camera\_sync.cc"} and \textit{"velodyne64\_sync.c"}. \textbf{"camera\_sync.cc"} is for cameras synchronization. It achieves synchronization by obtaining the cpu time, calculating the time that the trigger appears at different frequencies, and manipulating the GPIO port. The program will output the high / low level through the GPIO port at a specific time, and send the current ros timestamp at the same time through the ROS message. It supports the output of up to 5 different camera frequencies, and the output frequency can be within 1 $\sim$ 200Hz, which can be changed in \textit{"/launch/sync.launch"}. \textbf{"velodyne64\_sync.c"}'s workflow is similar to \textit{camera\_sync.cc}, but the difference is that it will start sending a 60ms high-level PPS every second to synchronize velodyne, and then send a simulated GPS sentence (\$GPRMC) through the serial port.
	
	
	\begin{figure}
		\centering
		\includegraphics[width=.4\textwidth]{600ms.jpeg}
		\caption{Photo at about 00:00:09.674}
		\label{600ms}
	\end{figure}
	
	\begin{figure}
		\centering
		\includegraphics[width=.4\textwidth]{700ms.jpeg}
		\caption{Photo at about 00:00:09.781}
		\label{700ms}
	\end{figure}
	
	
	\section{System Evaluation}
	\subsection{Sensors synchronization}
	In order to evaluate the sychronization system, experiment are designed. Refer to the \cite{Litos2006Synchronous}, we will make a LED array controlled by MircoChip to represent time. Let cameras to photograph the LED array and compare pictures from different cameras to visualize delay between cameras. For part of camera synchronization, we define it is a "good synchronization" as the relavite time differnece between two cameras is smaller than 1ms.
	\par
	For other sensors, such as IMU and LIDRA, cannot detect their time error by the approach from \cite{Litos2006Synchronous} in that their own sensor properties. So, we should update the experiment system. Because IMU detects the movement of object and LIDRA uses light beams to scan objects around, so besides the LED array, we will build a rotating strick like a windmill with IMU installed. Put different LIDRAs in front of the rotating strick to scan and achieve the  status of the system together with IMU. By comparing the different statuses from LIDRAs and IMU at the same time, we can evalue the synchronization system.
	\par
	However, there are some remaining problem of the experiment system. To compare system statuses from LIDRAs and IMU, system calibration is essential. But for now it is difficult to calibrate LIDRA and IMU.
	\par
	So far, we have done a simple test for camera synchronization. Fig \ref{600ms} and \ref{700ms} are two adjacent images about an online timer captured by the same camera. It shows that the time interval bewteen two images is about 107ms, which is close to 10Hz. Due to the limitations of the screen refresh rate and the accuracy of the network timer, we could not get the exact time difference. We also observed a blurred image from the image, which indicates that this measurement is not accurate. So next we will perform more accurate measurement according to the methods of \cite{Litos2006Synchronous} and \cite{8074435}.
	
	\subsection{Super Accuracy}
	This system using motion capture system as reference ground truth pose shall provide mapping results with great extent of accuracy improvement than the system using solely SLAM methods. The error of the map is measured by the root mean square (RMS) error compared to the ground truth map generated by FARO laser scanner.
	
	\section{Conclusions}
	For synchronization part, the synchronization system has been completed and worked well and we will perform a more accurate measurement in next stage.
	
	\begin{thebibliography}{00}
		\bibitem{ICL-NUIM} A. Handa, T. Whelan, J. McDonald, and A. J. Davison. A Benchmark for RGB-D Visual Odometry, 3D Reconstruction and SLAM. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages 1524–1531. IEEE, 2014
		\bibitem{Robot@Home} Ruiz-Sarmiento, J. R., Cipriano Galindo, and J. González-Jiménez. "Robot@ home, a robotic dataset for semantic mapping of home environments." The International Journal of Robotics Research 36.2 (2017): 131-141.
		\bibitem{TUM-RGBD} J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers. A Benchmark for the Evaluation of RGB-D SLAM Systems. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 573–580. IEEE, 2012
		\bibitem{EuRoC} M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W. Achtelik, and R. Siegwart. The EuRoC Micro Aerial Vehicle Datasets. The International Journal of Robotics Research (IJRR), 2016.
		\bibitem{KITTI} A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision Meets Robotics: The KITTI Dataset. The International Journal of Robotics Research (IJRR), 32(11):1231–1237, 2013
		\bibitem{Zurich_Urban_MAV} Andras Majdik and Charles Till and Davide Scaramuzza. The Zurich urban micro aerial vehicle dataset. The International Journal of Robotics Research (IJRR) 36 (2017): 269-273
		\bibitem{TMU-VI} Schubert, David, et al. "The TUM VI Benchmark for Evaluating Visual-Inertial Odometry." arXiv preprint arXiv:1804.06120 (2018).
		\bibitem{ADVIO} Cortés, S., Solin, A., Rahtu, E., \& Kannala, J. (2018). ADVIO: An authentic dataset for visual-inertial odometry. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 419-434).
		\bibitem{CoRBS} O. Wasenmuller, M. Meyer, and D. Stricker. CoRBS: Comprehensive RGB-D benchmark for SLAM using Kinect v2. In IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1–7. IEEE, 2016
		\bibitem{RobotCar} Maddern, Will, et al. "1 year, 1000 km: The Oxford RobotCar dataset." The International Journal of Robotics Research (2016): 0278364916679498. 
		\bibitem{Amundson2006Time}
		Isaac Amundson, Manish Kushwaha, Branislav Kusy, Peter Volgyesi, Gyula Simon,
		Xenofon Koutsoukos, and Akos Ledeczi.
		\newblock Time synchronization for multi-modal target tracking in heterogeneous
		sensor networks.
		\newblock {\em Social Science Electronic Publishing}, 11(2):152--4, 2006.
		
		\bibitem{Geiger2013IJRR}
		Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun.
		\newblock Vision meets robotics: The kitti dataset.
		\newblock {\em International Journal of Robotics Research (IJRR)}, 2013.
		
		\bibitem{1}
		h.~{Hu}, J.~{Wu}, and Z.~{Xiong}.
		\newblock A soft time synchronization framework for multi-sensors in autonomous
		localization and navigation.
		\newblock In {\em 2018 IEEE/ASME International Conference on Advanced
			Intelligent Mechatronics (AIM)}, pages 694--699, July 2018.
		
		\bibitem{Litos2006Synchronous}
		Georgios Litos, Xenophon Zabulis, and Georgios Triantafyllidis.
		\newblock Synchronous image acquisition based on network synchronization.
		\newblock In {\em Conference on Computer Vision \& Pattern Recognition
			Workshop}, 2006.
		
		\bibitem{Olson2010A}
		E.~Olson.
		\newblock A passive solution to the sensor synchronization problem.
		\newblock 2010.
		
		\bibitem{8074435}
		X.~{Ding} and P.~{Tao}.
		\newblock Synchronization detection of multicamera system based on led matrix.
		\newblock In {\em 2016 13th International Conference on Embedded Software and
			Systems (ICESS)}, pages 18--23, Aug 2016.
		
		\bibitem{zhao2019mapping}
		Xiting Zhao and Zhijie Yang and Sören Schwertfeger
		\newblock Mapping with Reflection -- Detection and Utilization of Reflection in 3D Lidar \newblock In {\em arXiv 1909.12483} 2019
		
	\end{thebibliography}
\end{document}

